# Chat models

## 概要

大規模言語モデル（LLM）は、テキスト生成、翻訳、要約、質問応答など、幅広い言語関連タスクにおいて優れた性能を発揮する高度な機械学習モデルです。シナリオごとにタスク固有の微調整を必要とせず、高い精度を実現します。

最新のLLMは通常、チャットモデルインターフェースを介してアクセスされます。このインターフェースは、[メッセージ](/docs/concepts/messages)のリストを入力として受け取り、[メッセージ](/docs/concepts/messages)を出力として返します。

最新世代のチャットモデルは、以下の追加機能を提供します。

* [ツール呼び出し](/docs/concepts/tool_calling): 多くの一般的なチャットモデルは、ネイティブの[ツール呼び出し](/docs/concepts/tool_calling) APIを提供しています。このAPIを使用すると、開発者はLLMが外部サービス、API、データベースと連携できるリッチアプリケーションを構築できます。ツール呼び出しは、非構造化データから構造化情報を抽出し、さまざまなタスクを実行するためにも使用できます。
* [構造化出力](/docs/concepts/structured_outputs): チャットモデルが、指定されたスキーマに一致するJSONなどの構造化形式で応答するようにする手法。
* [マルチモダリティ](/docs/concepts/multimodality): 画像、音声、動画など、テキスト以外のデータを扱う機能。

## 機能

LangChainは、異なるプロバイダーのチャットモデルを扱うための一貫したインターフェースを提供するとともに、LLMを使用するアプリケーションの監視、デバッグ、パフォーマンス最適化のための追加機能も提供します。

* 多数のチャットモデルプロバイダー（Anthropic、OpenAI、Ollama、Microsoft Azure、Google Vertex、Amazon Bedrock、Hugging Face、Cohere、Groqなど）との統合。サポートされているモデルの最新リストについては、[チャットモデル統合](/docs/integrations/chat/)をご覧ください。
* LangChainの[メッセージ](/docs/concepts/messages)形式またはOpenAI形式のいずれかを使用します。
* 標準の[ツール呼び出しAPI](/docs/concepts/tool_calling): ツールをモデルにバインドし、モデルからのツール呼び出しリクエストにアクセスし、ツールの結果をモデルに返すための標準インターフェースです。
* `with_structured_output` メソッドを介して[出力の構造化](/docs/concepts/structured_outputs/#structured-output-method)を行うための標準 API。
* [非同期プログラミング](/docs/concepts/async)、[効率的なバッチ処理](/docs/concepts/runnables/#optimized-parallel-execution-batch)、[豊富なストリーミング API](/docs/concepts/streaming) のサポートを提供します。
* LLM に基づく本番環境レベルのアプリケーションの監視とデバッグのための [LangSmith](https://docs.smith.langchain.com) との統合。
* 標準化された [トークンの使用](/docs/concepts/messages/#aimessage)、[レート制限](#rate-limiting)、[キャッシュ](#caching) などの追加機能。

## 統合

LangChainには、様々なプロバイダーの多様なモデルを利用できるチャットモデル統合が多数用意されています。

これらの統合には、以下の2種類があります。

1. **公式モデル**: LangChainまたはモデルプロバイダーによって公式にサポートされているモデルです。これらのモデルは、`langchain-<provider>`パッケージに含まれています。
2. **コミュニティモデル**: 主にコミュニティによって提供およびサポートされているモデルです。これらのモデルは、`langchain-community`パッケージに含まれています。

LangChainのチャットモデルは、クラス名に「Chat」を接頭辞として付ける命名規則に従って命名されています（例: `ChatOllama`、`ChatAnthropic`、`ChatOpenAI`など）。

サポートされているモデルの一覧については、[チャットモデル統合](/docs/integrations/chat/)をご覧ください。

:::note
名前に接頭辞「Chat」が**含まれていない**モデル、または名前に接尾辞として「LLM」が含まれているモデルは、通常、チャット モデル インターフェイスに従わず、代わりに文字列を入力として受け取り、文字列を出力として返すインターフェイスを使用する古いモデルを指します。
:::


## インターフェース

LangChain チャットモデルは [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) インターフェースを実装しています。`BaseChatModel` は [Runnable インターフェース](/docs/concepts/runnables) も実装しているため、チャットモデルは [標準ストリーミングインターフェース](/docs/concepts/streaming)、[非同期プログラミング](/docs/concepts/async)、最適化された [バッチ処理](/docs/concepts/runnables/#optimized-parallel-execution-batch) などをサポートします。詳細については、[Runnable インターフェース](/docs/concepts/runnables) を参照してください。

チャットモデルの主要なメソッドの多くは、[メッセージ](/docs/concepts/messages) を入力として操作し、出力としてメッセージを返します。

チャットモデルには、モデルの設定に使用できる標準的なパラメータセットが用意されています。これらのパラメータは通常、出力の温度、レスポンス内のトークンの最大数、レスポンスの最大待機時間など、モデルの動作を制御するために使用されます。詳細については、[標準パラメータ](#standard-parameters)セクションをご覧ください。

:::note
ドキュメントでは、「LLM」と「チャットモデル」という用語を同じ意味で使用することがよくあります。これは、ほとんどの最新のLLMがチャットモデルインターフェースを介してユーザーに公開されているためです。

ただし、LangChainには、チャットモデルインターフェースに従わず、文字列を入力として受け取り、文字列を出力するインターフェースを使用する、古いLLMの実装も存在します。これらのモデルは通常、「Chat」というプレフィックスなしで命名されます（例：`Ollama`、`Anthropic`、`OpenAI` など）。
これらのモデルは[BaseLLM](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.llms.BaseLLM.html#langchain_core.language_models.llms.BaseLLM)インターフェースを実装しており、"LLM"サフィックスが付く名前が付けられる場合があります（例：`OllamaLLM`、`AnthropicLLM`、`OpenAILLM`など）。通常、ユーザーはこれらのモデルを使用すべきではありません。
:::

### 主要なメソッド

チャットモデルの主要なメソッドは以下のとおりです。

1. **invoke**: チャットモデルを操作するための基本的なメソッドです。[メッセージ](/docs/concepts/messages)のリストを入力として受け取り、出力としてメッセージのリストを返します。
2. **stream**: チャットモデルの出力を生成時にストリーミングするメソッドです。
3. **batch**: チャットモデルへの複数のリクエストをまとめて処理することで、処理効率を高めるメソッドです。
4. **bind_tools**: ツールをチャットモデルにバインドし、モデルの実行コンテキストで使用できるようにするメソッドです。
5. **with_structured_output**: [構造化出力](/docs/concepts/structured_outputs)をネイティブにサポートするモデル用の `invoke` メソッドのラッパーです。

その他の重要なメソッドについては、[BaseChatModel API リファレンス](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) をご覧ください。

### 入力と出力

現代のLLMは通常、チャットモデルインターフェースを介してアクセスされます。このインターフェースは、[メッセージ](/docs/concepts/messages)を入力として受け取り、[メッセージ](/docs/concepts/messages)を出力として返します。メッセージは通常、役割（例：システム、人間、アシスタント）と、テキストまたはマルチモーダルデータ（例：画像、音声、動画）を含む1つ以上のコンテンツブロックに関連付けられます。

LangChainは、チャットモデルとのやり取りに2つのメッセージ形式をサポートしています。

1. **LangChainメッセージ形式**：LangChain独自のメッセージ形式で、デフォルトで使用され、LangChain内部で使用されます。
2. **OpenAIメッセージ形式**：OpenAIのメッセージ形式。

### 標準パラメータ

多くのチャットモデルには、モデルの設定に使用できる標準化されたパラメータがあります。

| Parameter      | Description                                                                                                                                                                                                                                                                                                    |
|----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `model`        | 使用する特定の AI モデルの名前または識別子 (例: `"gpt-3.5-turbo"` または `"gpt-4"`)。|
| `temperature`  | モデルの出力のランダム性を制御します。値が高いほど（例：1.0）、応答はより創造的になり、値が低いほど（例：0.0）、応答はより決定論的で焦点が絞られたものになります。|
| `timeout`      | リクエストをキャンセルする前にモデルからの応答を待つ最大時間（秒単位）。リクエストが無期限にハングアップしないようにします。|
| `max_tokens`   | レスポンス内のトークン（単語と句読点）の総数を制限します。これにより、出力の長さが制御されます。 |
| `stop`         | モデルがトークンの生成を停止するタイミングを示す停止シーケンスを指定します。例えば、レスポンスの終了を示す特定の文字列を使用できます。 |
| `max_retries`  | ネットワーク タイムアウトやレート制限などの問題によりリクエストが失敗した場合に、システムがリクエストの再送信を試行する最大回数。|
| `api_key`      | モデルプロバイダーへの認証に必要なAPIキー。通常、モデルへのアクセスをサインアップした際に発行されます。|
| `base_url`     | リクエストが送信されるAPIエンドポイントのURL。これは通常、モデルのプロバイダーによって提供され、リクエストを転送するために必要です。|
| `rate_limiter` | オプションの[BaseRateLimiter](https://python.langchain.com/api_reference/core/rate_limiters/langchain_core.rate_limiters.BaseRateLimiter.html#langchain_core.rate_limiters.BaseRateLimiter)は、レート制限の超過を回避するためにリクエストの間隔を空けます。詳細については、以下の[rate-limiting](#rate-limiting)を参照してください。 |

重要な注意事項：

- 標準パラメータは、意図された機能を持つパラメータを公開するモデルプロバイダにのみ適用されます。例えば、一部のプロバイダは最大出力トークンの設定を公開していないため、max_tokens はこれらのプロバイダではサポートされません。
- 標準パラメータは現在、独自の統合パッケージ（例：`langchain-openai`、`langchain-anthropic` など）を持つ統合にのみ適用され、`langchain-community` 内のモデルには適用されません。

チャットモデルは、その統合に固有の他のパラメータも受け入れます。チャットモデルでサポートされているすべてのパラメータを確認するには、そのモデルのそれぞれの [API リファレンス](https://python.langchain.com/api_reference/) を参照してください。

## ツールの呼び出し

チャットモデルは、[ツール](/docs/concepts/tools)を呼び出して、データベースからのデータの取得、APIリクエストの送信、カスタムコードの実行などのタスクを実行できます。詳細については、[ツールの呼び出し](/docs/concepts/tool_calling)ガイドをご覧ください。

## 構造化出力

チャットモデルは、特定の形式（JSONや特定のスキーマへの一致など）で応答するようにリクエストできます。この機能は、情報抽出タスクに非常に役立ちます。
この手法の詳細については、[構造化出力](/docs/concepts/structured_outputs)ガイドをご覧ください。

## マルチモーダル性

大規模言語モデル（LLM）はテキスト処理に限定されません。画像、音声、動画など、他の種類のデータの処理にも使用できます。これは[マルチモーダル性](/docs/concepts/multimodality)と呼ばれます。

現在、マルチモーダル入力をサポートしているのは一部のLLMのみで、マルチモーダル出力をサポートしているのはほぼありません。詳細については、各モデルのドキュメントをご覧ください。

## コンテキストウィンドウ

チャットモデルのコンテキストウィンドウとは、モデルが一度に処理できる入力シーケンスの最大サイズを指します。最新のLLMのコンテキストウィンドウはかなり大きいですが、チャットモデルを扱う際に開発者が留意すべき制限事項があります。

入力がコンテキストウィンドウを超えると、モデルは入力全体を処理できず、エラーが発生する可能性があります。会話型アプリケーションでは、コンテキストウィンドウによって会話を通してモデルが「記憶」できる情報量が決まるため、これは特に重要です。開発者は、制限を超えずに一貫性のある会話を維持するために、コンテキストウィンドウ内で入力を管理しなければならないことがよくあります。会話におけるメモリ処理の詳細については、[メモリ](https://langchain-ai.github.io/langgraph/concepts/memory/)を参照してください。

入力のサイズは、モデルが使用する処理単位である[トークン](/docs/concepts/tokens)で測定されます。

## 高度なトピック
 
### レート制限

多くのチャットモデルプロバイダーは、一定期間内に送信できるリクエスト数に制限を設けています。

レート制限に達すると、通常、プロバイダーからレート制限エラー応答が返され、それ以上リクエストを送信する前に待機する必要があります。

レート制限に対処するには、いくつかの方法があります。

1. リクエストの間隔を空けてレート制限の達成を回避する：チャットモデルは、初期化時に指定できる `rate_limiter` パラメータを受け入れます。このパラメータは、モデルプロバイダーへのリクエストのレートを制御するために使用されます。特定のモデルへのリクエストの間隔を空けることは、モデルのベンチマークを行いパフォーマンスを評価する際に特に有効な戦略です。この機能の使用方法の詳細については、[レート制限への対処方法](/docs/how_to/chat_model_rate_limiting/) をご覧ください。
2. レート制限エラーからの回復を試みる：レート制限エラーが発生した場合、リクエストを再試行する前に一定時間待つことができます。レート制限エラーが発生するたびに、待機時間を増やすことができます。チャットモデルには、再試行回数を制御できる `max_retries` パラメータがあります。詳細については、[標準パラメータ](#standard-parameters) セクションを参照してください。
3. 別のチャットモデルへのフォールバック: あるチャットモデルでレート制限に達した場合、レート制限のない別のチャットモデルに切り替えることができます。

### キャッシュ

チャットモデルAPIは遅くなる場合があるため、以前の会話の結果をキャッシュするかどうかという疑問が生じるのは当然です。理論的には、キャッシュはモデルプロバイダーへのリクエスト数を減らすことでパフォーマンスを向上させるのに役立ちます。しかし、実際には、チャットモデルのレスポンスのキャッシュは複雑な問題であり、慎重に検討する必要があります。

その理由は、モデルへの**正確な**入力をキャッシュする場合、会話の1回目または2回目のインタラクション後にキャッシュヒットが発生する可能性は低いからです。例えば、複数の会話が全く同じメッセージで始まる可能性はどれくらいあるでしょうか？全く同じメッセージが3つある場合はどうでしょうか？

別の方法として、セマンティックキャッシュを使用する方法があります。これは、入力そのものそのものではなく、入力の意味に基づいてレスポンスをキャッシュするものです。これは状況によっては効果的ですが、そうでない場合もあります。

セマンティックキャッシュは、アプリケーションのクリティカルパスにおいて別のモデルへの依存関係を導入します（例：セマンティックキャッシュは、テキストをベクター表現に変換するために[埋め込みモデル](/docs/concepts/embedding_models)に依存する場合があります）。また、入力の意味を正確に捉えられる保証はありません。

ただし、チャットモデルの応答をキャッシュすることが有益な状況もあります。例えば、よくある質問への回答に使用するチャットモデルがある場合、応答をキャッシュすることで、モデルプロバイダーの負荷とコストを軽減し、応答時間を短縮できます。

詳細については、[チャットモデルの応答をキャッシュする方法](/docs/how_to/chat_model_caching/)ガイドをご覧ください。

## 関連リソース

* チャットモデルの使用方法に関するガイド：[ハウツーガイド](/docs/how_to/#chat-models)。
* サポートされているチャットモデルの一覧：[チャットモデル統合](/docs/integrations/chat/)。

### 概念ガイド

* [メッセージ](/docs/concepts/messages)
* [ツール呼び出し](/docs/concepts/tool_calling)
* [マルチモダリティ](/docs/concepts/multimodality)
* [構造化出力](/docs/concepts/structured_outputs)
* [トークン](/docs/concepts/tokens)
