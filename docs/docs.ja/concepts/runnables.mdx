# Runnable インターフェース

Runnable インターフェースは、LangChain コンポーネントを操作するための基盤であり、[言語モデル](/docs/concepts/chat_models)、[出力パーサー](/docs/concepts/output_parsers)、[リトリーバー](/docs/concepts/retrievers)、[コンパイル済み LangGraph グラフ](
https://langchain-ai.github.io/langgraph/concepts/low_level/#compiling-your-graph) など、多くの LangChain コンポーネントに実装されています。

このガイドでは、開発者がさまざまな LangChain コンポーネントを一貫性と予測可能な方法で操作できるようにする Runnable インターフェースの主要な概念とメソッドについて説明します。

:::info 関連リソース
* ["Runnable" インターフェース API リファレンス](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable) では、Runnable インターフェースとそのメソッドの詳細な概要が提供されています。
* 組み込みの `Runnable` の一覧は、[LangChain Core API リファレンス](https://python.langchain.com/api_reference/core/runnables.html) に記載されています。これらの Runnable の多くは、[LangChain Expression Language (LCEL)](/docs/concepts/lcel) を使用して LangChain でカスタムの「チェーン」を作成する際に役立ちます。
:::

## Runnable インターフェースの概要

Runnable の方法は、Runnable コンポーネントで以下の処理を可能にする標準インターフェースを定義します。

* [呼び出し](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): 単一の入力を出力に変換します。
* [バッチ処理](/docs/how_to/lcel_cheatsheet/#batch-a-runnable): 複数の入力を効率的に出力に変換します。
* [ストリーミング](/docs/how_to/lcel_cheatsheet/#stream-a-runnable): 出力は生成された時点でストリーミングされます。
* 検査: Runnable の入力、出力、および構成に関するスキーマ情報にアクセスできます。
* 合成: [LangChain Expression Language (LCEL)](/docs/concepts/lcel) を使用して複数の Runnable を合成し、連携させることで複雑なパイプラインを作成できます。

Runnable インターフェースと LCEL 式に関連する一般的なパターンについては、[LCEL チートシート](/docs/how_to/lcel_cheatsheet) を確認してください。

<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>

LangChain Runnables は、複数の入力を並列処理できる組み込みの `batch` API（および `batch_as_completed` API）を提供しています。

これらのメソッドを使用すると、複数の独立した入力を処理する必要がある場合に、処理を順次ではなく並列に実行できるため、パフォーマンスを大幅に向上させることができます。

2 つのバッチ処理オプションは次のとおりです。

* `batch`: 複数の入力を並列処理し、入力と同じ順序で結果を返します。
* `batch_as_completed`: 複数の入力を並列処理し、完了した時点で結果を返します。結果は順序どおりに返されない場合がありますが、各結果にはマッチング用の入力インデックスが含まれます。

`batch` および `batch_as_completed` のデフォルト実装では、スレッドプール executor を使用して `invoke` メソッドを並列実行します。これにより、ユーザーがスレッドを管理する必要なく効率的な並列実行が可能になり、I/O 依存のコード（API リクエストの発行、ファイルの読み取りなど）を高速化できます。 Python の GIL (Global Interpreter Lock) が真の並列実行を妨げるため、CPU バウンドの操作にはそれほど効果的ではありません。

一部の Runnable は、特定のユースケース向けに最適化された独自の `batch` および `batch_as_completed` 実装を提供する場合があります (例: モデルプロバイダーが提供する `batch` API を利用する)。

:::note
`abatch` と `abatch_as_completed` の非同期バージョンは、asyncio の [gather](https://docs.python.org/3/library/asyncio-task.html#asyncio.gather) および [as_completed](https://docs.python.org/3/library/asyncio-task.html#asyncio.as_completed) 関数に依存して、`ainvoke` メソッドを並列に実行します。
:::

:::tip
`batch` または `batch_as_completed` を使用して大量の入力を処理する場合、並列呼び出しの最大数を制御したい場合があります。これは、`RunnableConfig` ディクショナリの `max_concurrency` 属性を設定することで実現できます。詳細については、[RunnableConfig](/docs/concepts/runnables/#runnableconfig) を参照してください。

チャットモデルには、リクエストのレートを制御できる組み込みの [レートリミッター](/docs/concepts/chat_models#rate-limiting) も備わっています。
:::

### Asynchronous support
<span data-heading-keywords="async-api"></span>

Runnable は非同期 API を公開しており、Python の `await` 構文を使用して呼び出すことができます。非同期メソッドは "a" プレフィックスで識別されます（例: `ainvoke`、`abatch`、`astream`、`abatch_as_completed`）。

詳細については、[LangChain を使用した非同期プログラミング](/docs/concepts/async) ガイドを参照してください。

## Streaming APIs
<span data-heading-keywords="streaming-api"></span>

LLM ベースのアプリケーションをエンドユーザーにとって応答性の高いものにするには、ストリーミングが不可欠です。

Runnable は次の 3 つのストリーミング API を公開します。

1. sync [stream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.stream) と async [astream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.astream): 生成された Runnable をそのまま出力します。
2. 非同期 `astream_events`: 中間ステップと最終出力をストリーミングできる、より高度なストリーミングAPI
3. **レガシー** 非同期 `astream_log`: 中間ステップと最終出力をストリーミングするレガシーストリーミングAPI

LangChainでのストリーミング方法の詳細については、[ストリーミング概念ガイド](/docs/concepts/streaming)を参照してください。

## 入力と出力の型

すべての `Runnable` は、入力と出力の型によって特徴付けられます。これらの入力と出力の型は任意の Python オブジェクトにすることができ、Runnable 自体によって定義されます。

Runnable の実行につながる Runnable メソッド (例: `invoke`、`batch`、`stream`、`astream_events`) は、これらの入力と出力の型で動作します。

* invoke: 入力を受け取り、出力を返します。
* batch: 入力のリストを受け取り、出力のリストを返します。
* stream: 入力を受け取り、出力を生成するジェネレーターを返します。

**入力型** と **出力型** はコンポーネントによって異なります。

| Component    | Input Type                                       | Output Type           |
|--------------|--------------------------------------------------|-----------------------|
| Prompt       | dictionary                                       | PromptValue           |
| ChatModel    | a string, list of chat messages or a PromptValue | ChatMessage           |
| LLM          | a string, list of chat messages or a PromptValue | String                |
| OutputParser | the output of an LLM or ChatModel                | Depends on the parser |
| Retriever    | a string                                         | List of Documents     |
| Tool         | a string or dictionary, depending on the tool    | Depends on the tool   |

入力と出力のタイプとその使用方法の詳細については、個々のコンポーネントのドキュメントを参照してください。

### スキーマの検査

:::note
これは高度な機能ですが、ほとんどのユーザーには不要です。
Runnableのスキーマを検査する必要がある場合を除き、このセクションはスキップすることをお勧めします。
:::

より高度なユースケースでは、Runnable をプログラムで **検査** し、Runnable が期待する入出力タイプと生成する入出力タイプを判断することが必要な場合があります。

Runnable インターフェースは、Runnable の入出力タイプの [JSON スキーマ](https://json-schema.org/) と、入出力タイプの [Pydantic スキーマ](https://docs.pydantic.dev/latest/) を取得するためのメソッドを提供します。

これらの API は主に、ユニットテストの内部処理や、入力検証や [OpenAPI ドキュメント](https://www.openapis.org/) の生成に API を使用する [LangServe](/docs/concepts/architecture#langserve) によって使用されます。

さらに、入出力タイプに加えて、一部の Runnable では追加の実行時構成オプションが設定されています。
Runnable の構成オプションの Pydantic スキーマと JSON スキーマを取得するための API も用意されています。詳細については、[構成可能なランナブル](#configurable-runnables)セクションを参照してください。

| Method                  | Description                                                      |
|-------------------------|------------------------------------------------------------------|
| `get_input_schema`      | Gives the Pydantic Schema of the input schema for the Runnable.  |
| `get_output_schema`     | Gives the Pydantic Schema of the output schema for the Runnable. |
| `config_schema`         | Gives the Pydantic Schema of the config schema for the Runnable. |
| `get_input_jsonschema`  | Gives the JSONSchema of the input schema for the Runnable.       |
| `get_output_jsonschema` | Gives the JSONSchema of the output schema for the Runnable.      |
| `get_config_jsonschema` | Gives the JSONSchema of the config schema for the Runnable.      |


#### With_types

LangChainは、利用可能な情報に基づいて、Runnableの入力型と出力型を自動的に推論します。

現在、この推論は[LCEL](/docs/concepts/lcel)コンポジションを使用して構築された複雑なRunnableでは適切に機能せず、推論された入力型や出力型が正しくない可能性があります。このような場合は、推論された入力型と出力型を`with_types`メソッド（[APIリファレンス](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.with_types)）を使用してオーバーライドすることをお勧めします。

## RunnableConfig

runnable の実行に使用されるメソッド（例：`invoke`、`batch`、`stream`、`astream_events`）はいずれも、`RunnableConfig` と呼ばれる2番目の引数を受け入れます（[APIリファレンス](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html#RunnableConfig)）。この引数は、runnable の実行時に使用される runnable の設定を含むディクショナリです。

`RunnableConfig` には、以下のプロパティを定義できます。

| Attribute       | Description                                                                                |
|-----------------|--------------------------------------------------------------------------------------------|
| run_name | 指定された Runnable に使用される名前 (継承されません)。 |
| run_id | この呼び出しの一意の識別子。サブ呼び出しにはそれぞれ一意の実行 ID が付与されます。 |
| tags | この呼び出しとサブ呼び出しのタグ。 |
| metadata | この呼び出しとサブ呼び出しのメタデータ。 |
| callbacks | この呼び出しとサブ呼び出しのコールバック。 |
| max_concurrency | 並列呼び出しの最大数 (例: バッチで使用)。 |
| recursion_limit | 呼び出しが再帰できる最大回数 (例: Runnable を返す Runnable で使用) |
| configurable | Runnable の構成可能な属性のランタイム値。 |

`config` を `invoke` メソッドに渡す方法は次の通りです:

```python
some_runnable.invoke(
   some_input, 
   config={
      'run_name': 'my_run', 
      'tags': ['tag1', 'tag2'], 
      'metadata': {'key': 'value'}
      
   }
)
```

### RunnableConfig の伝播

多くの `Runnable` は他の Runnable で構成されており、`RunnableConfig` が Runnable によって実行されるすべてのサブコールに伝播されることが重要です。これにより、親 Runnable に実行時設定値を提供し、すべてのサブコールに継承させることができます。

伝播されなければ、[コールバック](/docs/concepts/callbacks) や、すべてのサブコールに継承されることが期待される `tags` や `metadata` などの設定値の設定と伝播は不可能になります。

新しい `Runnable` を作成する主なパターンは 2 つあります。

1. [LangChain Expression Language (LCEL)](/docs/concepts/lcel) を用いた宣言的な方法:

    ```python
    chain = prompt | chat_model | output_parser
    ```

2. [カスタム Runnable](#custom-runnables) (例: `RunnableLambda`) を使用するか、`@tool` デコレータを使用する:

    ```python
    def foo(input):
        # Note that .invoke() is used directly here
        return bar_runnable.invoke(input)
    foo_runnable = RunnableLambda(foo)
    ```

LangChain は、どちらのパターンでも `RunnableConfig` を自動的に伝播しようとします。

2 番目のパターンを処理するために、LangChain は Python の [contextvars](https://docs.python.org/3/library/contextvars.html) に依存しています。

Python 3.11 以降では、これはそのまま動作し、サブ呼び出しに `RunnableConfig` を伝播するために特別な操作を行う必要はありません。

Python 3.9 および 3.10 では、**非同期コード** を使用している場合は、`Runnable` を呼び出す際に `RunnableConfig` を手動で渡す必要があります。

これは、Python 3.9 および 3.10 の [asyncio のタスク](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task) の制限により、`context` 引数が受け入れられなかったためです。

`RunnableConfig` を手動で伝播するには、次のようにします。

```python
async def foo(input, config): # <-- Note the config argument
    return await bar_runnable.ainvoke(input, config=config)
    
foo_runnable = RunnableLambda(foo)
```

:::caution
Python 3.10 以前を使用して非同期コードを記述する場合、`RunnableConfig` は自動的に伝播されないため、手動で伝播する必要があります。
これは、`astream_events` および `astream_log` を使用してデータをストリーミングしようとするときによくある落とし穴です。これらのメソッドは、`RunnableConfig` 内で定義された [コールバック](/docs/concepts/callbacks) が適切に伝播されることに依存しているためです。
:::

### カスタム実行名、タグ、メタデータの設定

`RunnableConfig` ディクショナリの `run_name`、`tags`、`metadata` 属性を使用して、特定の Runnable の実行名、タグ、メタデータにカスタム値を設定できます。

`run_name` は、実行にカスタム名を設定するために使用できる文字列です。この名前は、ログやその他の場所で実行を識別するために使用されます。サブコールには継承されません。

`tags` 属性と `metadata` 属性は、それぞれリストとディクショナリであり、実行にカスタムタグとメタデータを設定するために使用できます。これらの値はサブコールに継承されます。

これらの属性は [LangSmith](https://docs.smith.langchain.com/) でトレース属性として表示され、フィルタリングや検索に使用できるため、実行の追跡やデバッグに役立ちます。

属性は [コールバック](/docs/concepts/callbacks) にも伝播され、ストリーム内の各イベントの一部として [astream_events](/docs/concepts/streaming) などのストリーミング API に表示されます。

:::note 関連
* [LangChain を使ったトレース方法](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)
:::

### 実行IDの設定

:::note
これは高度な機能ですが、ほとんどのユーザーには不要です。
:::

後で参照したり、他のシステムと関連付けたりする場合に備えて、特定の実行にカスタムの `run_id` を設定する必要がある場合があります。

`run_id` は有効な UUID 文字列で、各実行で **一意** である必要があります。これは親実行を識別するために使用され、サブクラスは独自の一意の実行IDを自動的に取得します。

カスタムの `run_id` を設定するには、Runnable を呼び出す際に、`config` ディクショナリにキーと値のペアとして渡します。

```python
import uuid

run_id = uuid.uuid4()

some_runnable.invoke(
   some_input, 
   config={
      'run_id': run_id
   }
)

# Do something with the run_id
```

### 再帰回数の制限の設定

:::note
これは高度な機能ですが、ほとんどのユーザーには不要です。
:::

一部のRunnableは他のRunnableを返す場合があり、適切に処理しないと無限再帰が発生する可能性があります。これを防ぐには、`RunnableConfig`ディクショナリに`recursion_limit`を設定します。これにより、Runnableが再帰できる回数を制限できます。

### 最大同時実行数の設定

`batch` メソッドまたは `batch_as_completed` メソッドを使用する場合、`RunnableConfig` ディクショナリの `max_concurrency` 属性を設定することで、並列呼び出しの最大数を制御できます。これは、サーバーや API の過負荷を防ぐために並列呼び出しの数を制限したい場合に便利です。

:::tip
**チャットモデル** によるリクエスト数のレート制限を行う場合は、`max_concurrency` を設定する代わりに、組み込みの [レートリミッター](/docs/concepts/chat_models#rate-limiting) を使用するとより効果的です。

詳細については、[レート制限の処理方法](/docs/how_to/chat_model_rate_limiting/) ガイドをご覧ください。
:::

### configurable の設定

`configurable` フィールドは、Runnable の設定可能な属性に実行時の値を渡すために使用されます。

このフィールドは、[LangGraph](/docs/concepts/architecture#langgraph) の [LangGraph Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) および [memory](https://langchain-ai.github.io/langgraph/concepts/memory/) で頻繁に使用されます。

これは、[RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html#langchain_core.runnables.history.RunnableWithMessageHistory) でも同様の目的で使用され、会話履歴を追跡するための `session_id` または `conversation_id` を指定します。

さらに、これを使用して、作成する任意の [Configurable Runnable](#configurable-runnables) に渡すカスタム構成オプションを指定することもできます。

### コールバックの設定

このオプションを使用して、実行時にランナブルの[コールバック](/docs/concepts/callbacks)を設定します。設定したコールバックは、ランナブルによって実行されるすべてのサブコールに渡されます。

```python
some_runnable.invoke(
   some_input,
   {
      "callbacks": [
         SomeCallbackHandler(),
         AnotherCallbackHandler(),
      ]
   }
)
```

LangChain でコールバックを使用する方法の詳細については、[コールバックの概念ガイド](/docs/concepts/callbacks)をお読みください。

:::important
非同期環境で Python 3.9 または 3.10 を使用している場合、場合によっては `RunnableConfig` をサブコールに手動で伝播する必要があります。
詳細については、[RunnableConfig の伝播](#propagation-of-runnableconfig) セクションを参照してください。
:::

## 関数から runnable を作成する {#custom-runnables}

任意のロジックを実行するカスタム runnable を作成する必要がある場合があります。
これは、[LangChain Expression Language (LCEL)](/docs/concepts/lcel) を使用して複数の runnable を作成し、その中の1つのステップにカスタム処理ロジックを追加する必要がある場合に特に便利です。

関数からカスタム runnable を作成するには、2つの方法があります。

* `RunnableLambda`: ストリーミングが不要な単純な変換に使用します。
* `RunnableGenerator`: ストリーミングが必要な複雑な変換に使用します。

`RunnableLambda` と `RunnableGenerator` の使用方法の詳細については、[カスタム関数の実行方法](/docs/how_to/functions) ガイドをご覧ください。

:::important
ユーザーは、Runnable をサブクラス化して新しいカスタム Runnable を作成しようとしないでください。
これは、単純に `RunnableLambda` や `RunnableGenerator` を使用するよりもはるかに複雑で、エラーが発生しやすくなります。:::

## 設定可能な Runnable

:::note
これは高度な機能ですが、ほとんどのユーザーには不要です。

これは、[LangChain Expression Language (LCEL)](/docs/concepts/lcel) を使用して作成された大規模な「チェーン」の設定に役立ちます。
また、[LangServe](/docs/concepts/architecture#langserve) によってデプロイされた Runnable に活用されます。
:::

ランナブルで複数の異なる動作方法を試したり、エンドユーザーに公開したりしたい場合があります。これには、チャットモデルの温度などのパラメータを調整したり、異なるチャットモデルを切り替えたりすることが含まれます。

このプロセスを簡素化するために、Runnable インターフェースは、実行時に設定可能なランナブルを作成するための2つのメソッドを提供しています。

* `configurable_fields`: このメソッドを使用すると、ランナブルの特定の**属性**を設定できます。例えば、チャットモデルの `temperature` 属性などです。
* `configurable_alternatives`: このメソッドを使用すると、実行時に実行可能な**代替** Runnable を指定できます。例えば、使用可能な複数のチャットモデルのリストを指定できます。

ランタイムチェーンの内部設定方法の詳細については、[ランタイムチェーンの内部設定方法](/docs/how_to/configure)ガイドを参照してください。
