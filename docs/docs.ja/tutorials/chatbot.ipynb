{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "sidebar_position: 1\n",
    "keywords: [conversationchain]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# チャットボットを構築する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::note\n",
    "\n",
    "このチュートリアルでは以前、[RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) 抽象化を使用していました。このバージョンのドキュメントは [v0.2 ドキュメント](https://python.langchain.com/v0.2/docs/tutorials/chatbot/) でご覧いただけます。\n",
    "\n",
    "LangChain v0.3 リリース以降、LangChain ユーザーは [LangGraph 永続化](https://langchain-ai.github.io/langgraph/concepts/persistence/) を活用して、新しい LangChain アプリケーションに `memory` を組み込むことをお勧めします。\n",
    "\n",
    "既にコードで `RunnableWithMessageHistory` または `BaseChatMessageHistory` を使用している場合は、変更する必要はありません。この機能はシンプルなチャットアプリケーションでは機能するため、近い将来に廃止する予定はありません。また、`RunnableWithMessageHistory` を使用するコードは引き続き期待どおりに動作します。\n",
    "\n",
    "詳細については、[LangGraph Memory への移行方法](/docs/versions/migrating_memory/) をご覧ください。\n",
    "\n",
    ":::\n",
    "\n",
    "## 概要\n",
    "\n",
    "LLM を活用したチャットボットの設計と実装方法を例を挙げて説明します。\n",
    "\n",
    "このチャットボットは、[チャットモデル](/docs/concepts/chat_models) との会話が可能で、過去のやり取りを記憶します。\n",
    "構築するチャットボットは、会話を行うために言語モデルのみを使用する点に注意してください。\n",
    "他にも関連概念がいくつかありますので、ぜひご覧ください。\n",
    "\n",
    "- [会話型 RAG](/docs/tutorials/qa_chat_history): 外部データソースを介したチャットボットエクスペリエンスを実現する\n",
    "- [エージェント](/docs/tutorials/agents): アクションを実行できるチャットボットを構築する\n",
    "\n",
    "このチュートリアルでは、これら 2 つのより高度なトピックに役立つ基本事項を取り上げますが、必要に応じてそちらに直接進んでいただいても構いません。\n",
    "\n",
    "## セットアップ\n",
    "\n",
    "### Jupyter Notebook\n",
    "\n",
    "このガイド（およびドキュメント内の他のほとんどのガイド）では、[Jupyter Notebook](https://jupyter.org/) を使用し、読者も同様にJupyter Notebookを使用することを前提としています。Jupyter Notebookは、LLMシステムの使い方を学ぶのに最適です。なぜなら、予期しない出力やAPIのダウンなど、問題が発生することがよくあるからです。そのため、インタラクティブな環境でガイドを進めることで、より深く理解することができます。\n",
    "\n",
    "このチュートリアルやその他のチュートリアルは、Jupyter Notebookで実行するのが最も便利です。インストール方法については、[こちら](https://jupyter.org/install) をご覧ください。\n",
    "\n",
    "### インストール\n",
    "\n",
    "このチュートリアルでは、`langchain-core` と `langgraph` が必要です。このガイドでは `langgraph >= 0.2.28` が必要です。\n",
    "\n",
    "import Tabs from '@theme/Tabs';\n",
    "import TabItem from '@theme/TabItem';\n",
    "import CodeBlock from \"@theme/CodeBlock\";\n",
    "\n",
    "<Tabs>\n",
    "  <TabItem value=\"pip\" label=\"Pip\" default>\n",
    "    <CodeBlock language=\"bash\">pip install langchain-core langgraph>0.2.27</CodeBlock>\n",
    "  </TabItem>\n",
    "  <TabItem value=\"conda\" label=\"Conda\">\n",
    "    <CodeBlock language=\"bash\">conda install langchain-core langgraph>0.2.27 -c conda-forge</CodeBlock>\n",
    "  </TabItem>\n",
    "</Tabs>\n",
    "\n",
    "\n",
    "\n",
    "詳細については、[インストールガイド](/docs/how_to/installation)をご覧ください。\n",
    "\n",
    "### LangSmith\n",
    "\n",
    "LangChainで構築するアプリケーションの多くは、複数のステップでLLM呼び出しを複数回実行します。\n",
    "これらのアプリケーションが複雑になるにつれて、チェーンまたはエージェント内で何が起こっているかを正確に検査することが重要になります。\n",
    "これを行うには、[LangSmith](https://smith.langchain.com)を使用するのが最適です。\n",
    "\n",
    "上記のリンクでサインアップしたら、環境変数を設定してトレースのログ記録を開始してください:\n",
    "\n",
    "```shell\n",
    "export LANGSMITH_TRACING=\"true\"\n",
    "export LANGSMITH_API_KEY=\"...\"\n",
    "```\n",
    "\n",
    "または、ノートブックの場合は次のように設定できます:\n",
    "\n",
    "```python\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n",
    "```\n",
    "\n",
    "## クイックスタート\n",
    "\n",
    "まずは、言語モデル単体の使い方を学びましょう。LangChainは様々な言語モデルをサポートしており、相互に活用できます。以下からご希望の言語モデルを選択してください。\n",
    "\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs overrideParams={{openai: {model: \"gpt-4o-mini\"}}} />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずはモデルを直接使ってみましょう。`ChatModel`はLangChainの「Runnable」のインスタンスであり、操作するための標準インターフェースを公開しています。モデルを呼び出すには、`.invoke`メソッドにメッセージのリストを渡します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi Bob! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-5211544f-da9f-4325-8b8e-b3d92b2fc71a-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデル自体には状態の概念はありません。例えば、次のような質問をしてみましょう:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm sorry, but I don't have access to personal information about users unless it has been shared with me in the course of our conversation. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 11, 'total_tokens': 45, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-a2d13a18-7022-4784-b54f-f85c097d1075-0', usage_metadata={'input_tokens': 11, 'output_tokens': 34, 'total_tokens': 45, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LangSmith トレース](https://smith.langchain.com/public/5c21cb92-2814-4119-bae9-d02b8db577ac/r) の例を見てみましょう。\n",
    "\n",
    "前の会話の展開が考慮されておらず、質問に答えられないことがわかります。\n",
    "これはチャットボットのエクスペリエンスを著しく損なうものです。\n",
    "\n",
    "この問題を回避するには、[会話履歴](/docs/concepts/chat_history) 全体をモデルに渡す必要があります。そうするとどうなるか見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Bob! How can I help you today, Bob?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 33, 'total_tokens': 47, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-34bcccb3-446e-42f2-b1de-52c09936c02c-0', usage_metadata={'input_tokens': 33, 'output_tokens': 14, 'total_tokens': 47, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、良好な応答が得られていることがわかります！\n",
    "\n",
    "これが、チャットボットが会話形式でやり取りする能力を支える基本的な考え方です。\n",
    "では、これをどのように実装するのが最適でしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## メッセージの永続化\n",
    "\n",
    "[LangGraph](https://langchain-ai.github.io/langgraph/) は組み込みの永続化レイヤーを実装しており、複数の会話ターンをサポートするチャットアプリケーションに最適です。\n",
    "\n",
    "チャットモデルを最小限の LangGraph アプリケーションでラップすることで、メッセージ履歴を自動的に永続化できるため、マルチターンアプリケーションの開発が簡素化されます。\n",
    "\n",
    "LangGraph にはシンプルなメモリ内チェックポインタが付属しており、以下でこれを使用します。詳細については、[ドキュメント](https://langchain-ai.github.io/langgraph/concepts/persistence/) を参照してください。これには、SQLite や Postgres などのさまざまな永続化バックエンドの使用方法も含まれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、毎回実行可能ファイルに渡す `config` を作成する必要があります。この config には、入力ファイルに直接含まれない情報が含まれていますが、それでも役立ちます。今回の場合は、`thread_id` を含めます。これは以下のようになります:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これにより、単一のアプリケーションで複数の会話スレッドをサポートできるようになります。これは、アプリケーションに複数のユーザーがいる場合によくある要件です。\n",
    "\n",
    "次に、アプリケーションを起動します:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Bob! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi! I'm Bob.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob! How can I help you today, Bob?\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "素晴らしい！チャットボットが私たちに関する情報を記憶するようになりました。別の `thread_id` を参照するように設定を変更すると、会話が新しく開始されることがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I don't have access to personal information about you unless you've shared it in this conversation. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ただし、元の会話にいつでも戻ることができます（データベースに保存されているため）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob. What would you like to discuss today?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、多くのユーザーと会話するチャットボットをサポートできます。\n",
    "\n",
    ":::tip\n",
    "\n",
    "非同期サポートの場合は、`call_model` ノードを非同期関数に更新し、アプリケーションを呼び出すときに `.ainvoke` を使用します。\n",
    "\n",
    "```python\n",
    "# Async function for node:\n",
    "async def call_model(state: MessagesState):\n",
    "    response = await model.ainvoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define graph as before:\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# Async invocation:\n",
    "output = await app.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現時点では、モデルの周りにシンプルな永続化レイヤーを追加しただけです。プロンプトテンプレートを追加することで、チャットボットをより複雑でパーソナライズされたものにすることができます。\n",
    "\n",
    "## プロンプトテンプレート\n",
    "\n",
    "[プロンプトテンプレート](/docs/concepts/prompt_templates) は、生のユーザー情報を LLM が処理できる形式に変換するのに役立ちます。この場合、生のユーザー入力は単なるメッセージであり、LLM に渡されます。これをもう少し複雑にしてみましょう。まず、カスタム指示を含むシステムメッセージを追加します（ただし、メッセージは引き続き入力として受け取ります）。次に、メッセージ以外の入力も追加します。\n",
    "\n",
    "システムメッセージを追加するには、`ChatPromptTemplate` を作成します。`MessagesPlaceholder` を使用してすべてのメッセージを渡します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、このテンプレートを組み込むようにアプリケーションを更新できます:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    # highlight-start\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    # highlight-end\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同じ方法でアプリケーションを呼び出します:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ahoy there, Jim! What brings ye to these waters today? Be ye seekin' treasure, knowledge, or perhaps a good tale from the high seas? Arrr!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"Hi! I'm Jim.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ye be called Jim, matey! A fine name fer a swashbuckler such as yerself! What else can I do fer ye? Arrr!\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "素晴らしいですね！では、プロンプトをもう少し複雑にしてみましょう。プロンプトテンプレートが以下のようになっていると仮定しましょう:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "プロンプトに新しい `language` 入力を追加したことに注目してください。アプリケーションには現在、入力の `messages` と `language` という 2 つのパラメータがあります。これを反映するために、アプリケーションの状態を更新する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "# highlight-next-line\n",
    "class State(TypedDict):\n",
    "    # highlight-next-line\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    # highlight-next-line\n",
    "    language: str\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "¡Hola, Bob! ¿Cómo puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"Hi! I'm Bob.\"\n",
    "language = \"Spanish\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    # highlight-next-line\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状態全体が永続化されるため、変更が不要な場合は `language` などのパラメータを省略できることに注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Tu nombre es Bob. ¿Hay algo más en lo que pueda ayudarte?\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "内部で何が起こっているかを理解するには、[この LangSmith トレース](https://smith.langchain.com/public/15bd8589-005c-4812-b9b9-23e74ba4c3c6/r) を確認してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 会話履歴の管理\n",
    "\n",
    "チャットボットを構築する際に理解しておくべき重要な概念の一つは、会話履歴の管理方法です。管理を行わないと、メッセージリストが無制限に大きくなり、LLMのコンテキストウィンドウをオーバーフローする可能性があります。そのため、渡すメッセージのサイズを制限するステップを追加することが重要です。\n",
    "\n",
    "**重要なのは、この処理はプロンプトテンプレートの前、かつメッセージ履歴から以前のメッセージを読み込んだ後に行う必要があるということです。**\n",
    "\n",
    "これを実現するには、プロンプトの前に `messages` キーを適切に変更する簡単なステップを追加し、その新しいチェーンをメッセージ履歴クラスでラップします。\n",
    "\n",
    "LangChainには、[メッセージリストの管理](/docs/how_to/#messages)用のヘルパーがいくつか組み込まれています。今回は、[trim_messages](/docs/how_to/trim_messages/)ヘルパーを使用して、モデルに送信するメッセージの数を減らします。トリマーを使用すると、保持するトークンの数を指定できるほか、システム メッセージを常に保持するかどうかや部分的なメッセージを許可するかどうかなどの他のパラメータも指定できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これをチェーン内で使用するには、プロンプトに `messages` 入力を渡す前にトリマーを実行する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    # highlight-start\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    # highlight-end\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでモデルに名前を尋ねても、チャット履歴のその部分をトリミングしたため、モデルは名前を認識できません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't know your name. You haven't told me yet!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
    "query = \"What is my name?\"\n",
    "language = \"English\"\n",
    "\n",
    "# highlight-next-line\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しかし、最後の数個のメッセージ内の情報について尋ねると、次のことが記憶されます:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You asked what 2 + 2 equals.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
    "query = \"What math problem did I ask?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangSmith を見れば、[LangSmith トレース](https://smith.langchain.com/public/04402eaa-29e6-4bb1-aa91-885b730b6c21/r) で内部で何が起こっているかを正確に確認できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ストリーミング\n",
    "\n",
    "これでチャットボットが機能するようになりました。しかし、チャットボットアプリケーションにおいて*本当に*重要なUXの考慮事項の一つはストリーミングです。LLMは応答に時間がかかることがあるため、ユーザーエクスペリエンスを向上させるために、多くのアプリケーションでは、生成されたトークンをストリーミングで返しています。これにより、ユーザーは進行状況を確認できます。\n",
    "\n",
    "これは実はとても簡単です！\n",
    "\n",
    "デフォルトでは、LangGraphアプリケーションの`.stream`はアプリケーションのステップ（この場合は、モデルレスポンスの1ステップ）をストリーミングします。`stream_mode=\"messages\"`を設定すると、出力トークンをストリーミングできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Hi| Todd|!| Here|’s| a| joke| for| you|:\n",
      "\n",
      "|Why| don|’t| skeleton|s| fight| each| other|?\n",
      "\n",
      "|Because| they| don|’t| have| the| guts|!||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"Hi I'm Todd, please tell me a joke.\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "# highlight-next-line\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 次のステップ\n",
    "\n",
    "LangChain でチャットボットを作成する方法の基本を理解したら、より高度なチュートリアルもご覧ください。\n",
    "\n",
    "- [会話型 RAG](/docs/tutorials/qa_chat_history): 外部データソースを介してチャットボットエクスペリエンスを実現します。\n",
    "- [エージェント](/docs/tutorials/agents): アクションを実行できるチャットボットを構築します。\n",
    "\n",
    "さらに詳しい内容を知りたい場合は、以下のチュートリアルもご覧ください。\n",
    "\n",
    "- [ストリーミング](/docs/how_to/streaming): ストリーミングはチャットアプリケーションにとって*極めて重要*です。\n",
    "- [メッセージ履歴の追加方法](/docs/how_to/message_history): メッセージ履歴に関するあらゆる詳細について解説します。\n",
    "- [大量のメッセージ履歴の管理方法](/docs/how_to/trim_messages/): 大量のチャット履歴を管理するためのその他のテクニック\n",
    "- [LangGraph メインdocs](https://langchain-ai.github.io/langgraph/): LangGraphを使ったビルドの詳細については、"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
